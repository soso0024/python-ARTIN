{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `points = np.random.randn(100, 2)`\n",
    "そのコード行は標準正規分布（平均0、標準偏差1の正規分布）からランダムに選ばれた値を持つ100点のデータを2次元空間で生成しています。具体的には、`np.random.randn(100, 2)` は100行2列の配列を生成します。各行は2次元空間における点を表し、合計で100点のデータが生成されます。\n",
    "\n",
    "ただし、この方法で生成されたデータはクラスタに分かれていないランダムな点の集合です。`make_blobs`のように事前に定義されたクラスタ中心点を持つわけではなく、点は独立しており、何らかのクラスタ構造はありません。\n",
    "### `points = np.random.randn(5, 2) + np.array([5, 0])`\n",
    "\n",
    "このコード行は、次の2つのステップを行っています。\n",
    "\n",
    "1. `np.random.randn(5, 2)`は、標準正規分布（平均0、標準偏差1の正規分布）からランダムに選ばれた値を持つ5点のデータを2次元空間で生成します。\n",
    "2. `+ np.array([5, 0])`は、生成された各点に対して、第1次元（x座標）に5を加算し、第2次元（y座標）には0を加算します。\n",
    "\n",
    "これにより、元の中心が(0,0)だったデータのクラウドが、新しい中心が(5,0)になるように移動します。つまり、このコードは平均が(5,0)の正規分布に従う5つのランダムな2次元点を生成します。このようにして、データセット内の点を特定の位置に「シフト」することができます。\n",
    "\n",
    "これは、単一のクラスタの中心を(5,0)に設定する場合に役立ちます。複数のクラスタを生成する場合は、異なる中心点を持つ複数のこのような点の集合を生成し、それらを結合することで、全体として複数のクラスタを持つデータセットを作ることができます。\n",
    "### `points = np.r_[points1, points2, points3, points4]`\n",
    "`point1`, `point2`, `point3`, `point4` はそれぞれ異なる中心点を持つデータセットです。このコード行は、それらを結合して、4つのクラスタを持つデータセットを作成します。\n",
    "### 参考文献\n",
    "- [k-means法を理解する](https://qiita.com/g-k/items/0d5d22a12a4507ecbf11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `[: , 0], [: , 1]` とは\n",
    "\n",
    "`points` は \\( N \\times 2 \\) の形状を持つ配列で、ここで \\( N \\) は点の数です。この配列の各行は2次元空間におけるデータポイントを表しています。\n",
    "\n",
    "- `points[:, 0]`: これは `points` 配列のすべての行における第1列の要素を取得します。これは2次元空間における各点の x 座標を表しています。\n",
    "\n",
    "- `points[:, 1]`: これは `points` 配列のすべての行における第2列の要素を取得します。これは2次元空間における各点の y 座標を表しています。\n",
    "\n",
    "`matplotlib` の `scatter` 関数を使ってこれらの点をプロットするとき、`points[:, 0]` は x 軸の値として、`points[:, 1]` は y 軸の値として使用され、結果として2次元平面上に点が表示されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `plt.scatter` 関数\n",
    "\n",
    "`plt.scatter` 関数において引数 `c` は、プロットされる各データポイントの色を指定するために使われます。`c=y` の場合、`y` は各データポイントに対応する色情報を含む配列またはシーケンスを指すと考えられます。\n",
    "\n",
    "ここで、`y` の各要素は、データポイントが属するクラスタまたはカテゴリを表す数値である可能性が高く、`plt.scatter` はこの情報に基づいて異なる色でデータポイントをプロットします。これにより、クラスタリングの結果を視覚的に区別しやすくなります。\n",
    "\n",
    "例えば、`y` が `[0, 1, 0, 1, 2, 2]` のような配列だった場合、`plt.scatter` は最初と3番目の点を1つの色で、2番目と4番目の点を別の色で、5番目と6番目の点をさらに別の色でプロットします。この `y` 配列は通常、クラスタリングアルゴリズムやラベル付けプロセスから得られるものです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### シフトされたクラスター\n",
    "\n",
    "整数を加算することでクラスターを分ける理由は、それにより各クラスターのデータポイントが2次元空間において異なる位置に「シフト」されるからです。ここでの目的は、各クラスターが互いに十分な距離を持って分離されるようにすることです。\n",
    "\n",
    "例えば、`points1` には何も加算されていませんので、これらのデータポイントは原点近くに集中する傾向があります。一方、`points2`, `points3`, `points4` にはそれぞれ異なる大きさのオフセットが加算されており、これによりそれぞれのクラスターが異なる中心点を持つようになります。\n",
    "\n",
    "- `points2` は `points1` に比べて、x軸とy軸の両方に10から19のランダムな値を加算します（10がベースに加わり、さらに1から9がランダムに加わるため）。\n",
    "- `points3` は `points1` に比べて、x軸とy軸の両方に20から29のランダムな値を加算します。\n",
    "- `points4` は `points1` に比べて、x軸とy軸の両方に30から39のランダムな値を加算します。\n",
    "\n",
    "これにより、各グループの点が2次元空間において互いに異なる位置に配置され、クラスタリングされたデータセットが生成されます。実際には、これらのオフセットが十分に大きい場合（つまり、クラスターの中心が互いに十分離れている場合）、クラスター間の重複が少なくなり、クラスタリングアルゴリズムが異なるクラスターをより簡単に識別できるようになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `for i in range(n):`\n",
    "\n",
    "この行のコード `for j in range(i + 1, n):` は、Pythonのforループを使用して、特定の範囲のインデックスに対して繰り返し処理を行うためのものです。ここでの目的は、ラベルのペアを比較して、ランド指数を計算する際に必要なカウント（`a` と `b`）を集計することです。\n",
    "\n",
    "具体的には、以下のような処理を行っています：\n",
    "\n",
    "- `i` は外側のループで使用されるインデックスで、`labels_true` および `labels_pred` 配列の各要素を順に参照します。\n",
    "- `j` は内側のループで使用されるインデックスで、`i` の次の要素から配列の最後までを参照します。\n",
    "\n",
    "`for j in range(i + 1, n):` のループは、`i` 番目の要素とそれ以降の要素（`i+1` から `n-1` まで）を比較するために使われます。これにより、配列内の全てのユニークなペアの組み合わせを一度ずつ比較することができます。\n",
    "\n",
    "たとえば、配列が `[1, 2, 3, 4]` の4つの要素を持っている場合、以下のペアが比較されます：\n",
    "\n",
    "- `i=0` のとき: (1, 2), (1, 3), (1, 4)\n",
    "- `i=1` のとき: (2, 3), (2, 4)\n",
    "- `i=2` のとき: (3, 4)\n",
    "\n",
    "このようにして、`i` が増えるにつれて比較するペアの数が減っていきますが、全ての可能なペアがちょうど一度ずつ比較されるようになっています。これは、ランド指数を計算する際に、各ペアが同じクラスタに属しているか（`a` のカウント）、異なるクラスタに属しているか（`b` のカウント）を判断するために必要です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pdist` と `squareform`\n",
    "\n",
    "`pdist` 関数と `squareform` 関数を使用するときの違いは次のとおりです。\n",
    "\n",
    "1. `pdist(points, 'euclidean')` を実行すると、与えられた `points` 配列の全てのペアの点間でユークリッド距離を計算し、結果を一次元配列で返します。この配列は「コンデンスされた距離行列」と呼ばれ、\\( n \\) 個のデータポイントの間に \\( n \\times (n - 1) / 2 \\) 個のユニークなペアワイズ距離が含まれます。\n",
    "\n",
    "2. `squareform(pdist(points, 'euclidean'))` を実行すると、`pdist` によって生成された一次元配列を取り、それを二次元の「正方距離行列」に変換します。この行列は \\( n \\times n \\) のサイズを持ち、行と列がそれぞれ `points` 配列内のデータポイントに対応しています。行列の各要素は、対応するデータポイントのペア間のユークリッド距離を示します。この行列は対称であり、対角線上の要素は全てゼロです（なぜなら点自体との距離はゼロだからです）。\n",
    "\n",
    "要するに、`pdist` だけの結果はメモリ効率が良く、必要な距離情報を全て含んでいますが、直接的なインデックスアクセスができないコンパクトな形式です。一方で `squareform(pdist(...))` の結果は、通常の二次元配列の形で距離を格納しており、より直感的なインデックスアクセスが可能ですが、対称性のために冗長な情報を含んでおりメモリ使用量が多くなります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ユークリッド距離の計算\n",
    "\n",
    "\\( n \\) 個のデータポイントの間に \\( \\frac{n \\times (n - 1)}{2} \\) 個のユニークなペアワイズ距離があるとは、各データポイントが他の全てのデータポイントと一度ずつペアを組んで距離が計算されることを意味します。\n",
    "\n",
    "たとえば、データポイントが4つある場合を考えてみましょう。これらのデータポイントをA、B、C、Dとします。AとB、AとC、AとDのように、Aは他の3つのデータポイントとそれぞれペアを形成します。次に、BはAとのペアは既に形成されているため、CとDとペアを形成します。これを続けると、次のペアが得られます。\n",
    "\n",
    "- A-B\n",
    "- A-C\n",
    "- A-D\n",
    "- B-C\n",
    "- B-D\n",
    "- C-D\n",
    "\n",
    "これは合計で6つのペアであり、\\( \\frac{4 \\times (4 - 1)}{2} = \\frac{4 \\times 3}{2} = 6 \\) と計算できます。この計算方法は、すべての可能なペアの組み合わせを数え上げる際に、重複を避けるために使われます。なぜなら、B-AはA-Bと同じペアであり、二度数える必要がないからです。\n",
    "\n",
    "この原則は任意の \\( n \\) に対しても適用され、\\( n \\) 個のデータポイント間の全てのユニークなペアワイズ距離の数は \\( \\frac{n \\times (n - 1)}{2} \\) になります。この値は、`pdist` 関数が返すコンデンスされた距離行列の要素数と一致します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### クラスター分析\n",
    "\n",
    "- [クラスター分析](https://www.macromill.com/service/data-analysis/cluster-analysis/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 決定論的なアルゴリズム\n",
    "\n",
    "決定論的なアルゴリズム（Deterministic Algorithm）とは、同じ入力に対して必ず同じ出力を生成するアルゴリズムのことを指します。この種のアルゴリズムでは、ランダム性や確率的な要素は含まれておらず、処理の結果は完全に予測可能です。\n",
    "\n",
    "決定論的アルゴリズムの特徴は以下の通りです：\n",
    "\n",
    "1. **予測可能性**: 同じ入力データに対してアルゴリズムを複数回実行した場合、結果は常に同じになります。\n",
    "\n",
    "2. **再現性**: 処理の結果は入力データに完全に依存しているため、同じ条件下であればどの環境でも同じ結果が得られます。\n",
    "\n",
    "3. **確実性**: アルゴリズムの動作や結果にランダム性や不確実性がないため、出力は確実に予測できます。\n",
    "\n",
    "たとえば、基本的な数学の演算（加算、減算など）、ソートアルゴリズム（クイックソート、マージソートなど）、そして多くのクラスタリングアルゴリズム（K-means、階層的クラスタリングなど）は決定論的です。これらのアルゴリズムは、同じ入力データに対して常に一貫した結果を返します。\n",
    "\n",
    "対照的に、確率論的なアルゴリズム（Stochastic Algorithm）やランダム化アルゴリズムは、同じ入力に対しても異なる結果を生成する可能性があります。これらは、アルゴリズム内部でランダムな選択や確率的な決定を行うため、同じ入力データでも実行ごとに異なる結果を得ることがあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
